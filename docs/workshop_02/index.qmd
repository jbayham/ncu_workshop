---
title: "Workshop 02: Introduction to Processing Raster and Vector Data"
author: "Jude Bayham"
format: 
  html:
    theme: zephyr
    toc: true
---

<!-- ![](banner.png){width="100%"} -->

The objective of this workshop is to introduce the concept of processing raster and vector data. We will read in and visualize land cover data in Japan. Then we will use spatial operations to summarize raster information around points and inside of polygons.

## Prerequisites

### Reading

Chapters 4 and 6 from [R as GIS for Economists](https://tmieno2.github.io/R-as-GIS-for-Economists-Quarto/) will help you prepare for the workshop.

### Land Cover Data

We will use [High-Resolution Land-Use and Land-Cover](https://www.eorc.jaxa.jp/ALOS/en/dataset/lulc_e.htm) data from the Japan Aerospace Exploration Agency (JAXA). The data is published in various resolutions. We will use the 250 meter resolution data for illustration, but the code we write could be easily adapted for the higher-resolution versions of the data. JAXA does request that you register to download their data. The datasets we will use are posted here for ease:

-   [JAXA_HRLULC_Japan_v23.12_250m.tif (2022)](inputs/JAXA_HRLULC_Japan_v23.12_250m.tif)

-   [JAXA_HRLULC_Japan_v21.11_250m.tif (2018 - 2020)](inputs/JAXA_HRLULC_Japan_v21.11_250m.tif)

### RStudio

We will be working with packages designed to process and visualize geospatial data. You can install a package by typing the following into the console: `install.packages("package_name")`. *Note the quotations*. Please install the following packages on the computer you will be using:

-   `sf`: simple features a package with many utilities for working with spatial vector data
-   `raster`: a package for working with raster data
-   `terra`: another package for working with raster data
-   `stars`: a package for working with spatiotemporal data
-   `exactextractr`: an optimized package for extracting and aggregating raster data to polygons
-   `tmap`: a package for creating interactive maps of vector and raster data

*Note that you do not need to install packages if they are already installed - you only need to install packages once.*

## Getting started

Before opening Rstudio and working with the data, you should organize your digital workspace.

1.  Create a directory for this workshop titled: `workshop_02`. If you already have a directory for this class/seminar, create the directory under the class directory.

2.  Create a directory titled `inputs` under `workshop_02`. The path should be `workshop_02/inputs`.

3.  Download and move the [Land Cover Data] into `workshop_02/inputs`.

4.  We are also going to use the Japan political boundaries file from last week. Locate that file and copy it into `workshop_02/inputs`.

Open up Rstudio and do the following:

1.  Open a new script.

2.  Write a comment with a brief description about what the script does. In most cases, you know the intention of the script.

3.  Load the package `pacman` and use `p_load()` to install and load the packages we will be using in this workshop.

4.  Navigate to `workshop_02` (the directory you just created for the project). You can use the dropdown menu Session \> Set Working Directory \> Choose Directory or type the `setwd("path_here")` command at the top of your script. If you use the dropdown menu, R will generate the `setwd()` command and display it in the console. Copy and paste it into your script.

5.  Save your script and title the file: `workshop_02.R`

The first several lines of your script should look something like this:

```{r}
#| eval: false
#This is an introduction to working with spatial data in R

library(pacman) #if you have never used this package, you need to install it: install.packages("pacman)
pacman::p_load(tidyverse,sf,raster,terra,stars,exactextractr,tmap,measurements,fixest,janitor)

setwd("your_path/workshop_02")

######################
```

```{r}
#| echo: false

library(pacman) #if you have never used this package, you need to install it: install.packages("pacman)
pacman::p_load(tidyverse,sf,raster,terra,stars,exactextractr,tmap,measurements,fixest,janitor)

```

## Reading spatial data

We will primarily be using two spatial layers in this workshop. The first is the prefecture layer that we created in workshop 1. The second is the [Land Cover Data] described above. Let's start by reading in the land cover layer.

### Step 1: Loading and Preparing Raster Data

We begin by loading a raster dataset representing land cover in Japan. `terra::rast()` is used to load raster data efficiently.

```{r}
# Read in land cover raster data
jp_lc <- terra::rast("inputs/JAXA_HRLULC_Japan_v23.12_250m.tif")
```

### Step 2: Converting Coordinates to a Spatial Object

Here, we define a point of interest using its latitude and longitude, then convert it into a spatial object. `st_as_sf()` transforms a data frame into a spatial object. Coordinate Reference System (CRS) 4326 represents WGS84 (used in GPS).

```{r}
# Define a point with latitude and longitude
ncu_point <- data.frame(lat = 35.138496, lon = 136.925901) %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326) # Convert to spatial object
```

### Step 3: Creating a Buffer Around the Point

We buffer the point with a radius of 50 km. Buffers are used to analyze spatial relationships within a specified distance.

```{r}
# Create a 50 km buffer around the point
ncu_buffered <- ncu_point %>%
  st_buffer(dist = 50000) # Distance is in meters for CRS 4326
```

### Step 4: Cropping the Raster to the Buffer Area

We crop the raster data to the extent of the buffered area to focus the analysis. `terra::crop()` reduces the extent of the raster to the specified area, making spatial operations more efficient.

```{r}
# Crop raster layer to the buffered area
jp_lc_cropped <- terra::crop(jp_lc, ncu_buffered)
```

### Step 5: Visualizing the Data

We use `tmap` to create an interactive map.

```{r}
# Create a map visualization
ncu_map_rast <- tm_basemap("CartoDB.Positron") +  # Add a basemap
  tm_basemap("Esri.WorldImagery") +              # Add satellite imagery
  tm_shape(ncu_point) +                          # Add the point
  tm_symbols(col = "red", size = 0.001) +        # Style the point
  tm_shape(ncu_buffered) +                       # Add the buffer
  tm_borders(col = "darkred") +                  # Style the buffer
  tm_shape(jp_lc_cropped) +                      # Add the cropped raster
  tm_raster()                                    # Display the raster

# Render the map
tmap_leaflet(ncu_map_rast)
```

### Step 6: Assigning a Color Palette to Categories

We define land cover categories and assign colors to make the map intuitive. I gathered these from the JAXA documentation.

```{r}
# Define categories and colors
categories <- c("Water bodies", "Built-up", "Paddy field", "Cropland", "Grassland",
                "Deciduous Broad-leaved Forest (DBF)", "Deciduous Needle-leaved Forest (DNF)",
                "Evergreen Broad-leaved Forest (EBF)", "Evergreen Needle-leaved Forest (ENF)", 
                "Bare", "Bamboo forest", "Solar panel", "Wetland", "Greenhouse")

colors <- c("#76C1FF", "#D3D3D3", "#FFD966", "#FFA07A", "#98FB98",
            "#228B22", "#556B2F", "#006400", "#2E8B57", "#F5F5DC",
            "#8B4513", "#D01A11", "#4682B4", "#ADFF2F")

# Assign categories to the raster
cat_ref <- data.frame(ID = 1:length(categories), label = categories)
levels(jp_lc_cropped) <- cat_ref
```

### Step 7: Adding Color-Coded Raster to the Map

We re-visualize the data with the defined color palette.

```{r}
# Update map with color-coded raster
ncu_map_rast <- tm_basemap("CartoDB.Positron") +  # Add basemap
  tm_basemap("Esri.WorldImagery") +
  tm_shape(ncu_point) + 
  tm_symbols(col = "red", size = 0.001) +
  tm_shape(ncu_buffered) +
  tm_borders(col = "darkred") +
  tm_shape(jp_lc_cropped) +
  tm_raster(palette = colors, alpha = 0.5, style = "cat")

# Render the updated map
tmap_leaflet(ncu_map_rast)
```

### Step 8: Calculating Area by Land Cover Type

We use the `exactextractr` package to extract raster values within the buffered area and calculate the area corresponding to each land type.

```{r}
# Calculate area of each land cover type within the buffer
# Step 1: Extract raster values and weights within the buffer
extraction <- exact_extract(jp_lc_cropped, ncu_buffered, include_area = TRUE)

# Step 2: Summarize the results by land cover type
area_summary <- extraction[[1]] %>%
  group_by(value) %>% # Group by land cover type (value)
  summarize(total_area_m2 = sum(coverage_fraction * area, na.rm = TRUE)) # Weighted area in mÂ²

# Step 3: Merge with category labels for clarity
area_summary <- area_summary %>%
  left_join(cat_ref, by = c("value" = "ID")) %>% # Join with category labels
  dplyr::select(label, total_area_m2) %>% # Select relevant columns
  arrange(desc(total_area_m2)) # Arrange by area in descending order

# View the summarized area table
print(area_summary)
```

------------------------------------------------------------------------

### Explanation of the Code:

1.  **`exact_extract`**: Extracts raster values and the fraction of each raster cell covered by the buffer area. Setting `include_area=TRUE` allows calculating the area in square meters.
2.  **`coverage_fraction * area`**: Multiplies the fraction of the cell within the buffer by the cell's area to compute the weighted area for each land type.
3.  **`group_by` and `summarize`**: Aggregates the total area by land cover type.
4.  **`left_join` with `cat_ref`**: Adds descriptive land cover labels to the results for better interpretation.

## Solar Panel Coverage

As I looked at this data, I wondered weather area covered by solar panels has increased over time. Only the last two versions of the land cover data contain solar panel classification, so we will use the past two versions to compare prefecture level

### Step 1: Loading and Preparing the Data

First, we load vector data representing Japan's prefecture boundaries. Let's transform the prefecture data to the same CRS as the land cover data. In this case, transforming the vector data is more efficient than the raster data.

```{r}
# Read Japan prefecture boundaries
pref <- st_read("inputs/japan_prefecture.gpkg") %>%
  st_transform(st_crs(jp_lc))
```

Then, let's load the two raster layers

```{r}
#We've already loaded the first layer, so let's just rename it for clarity
jp_lc_2022 = jp_lc
#Connect to 2018 land cover data
jp_lc_2018 <- terra::rast("inputs/JAXA_HRLULC_Japan_v21.11_250m.tif")
```

### Step 2: Extract

We have loaded/connected to polygons of the prefectures and land cover raster data. Recall that the raster cells contain an integer value representing the land cover classification. We want to extract all of the values within each polygon in `pref`. `exactextractr::exact_extract()` is designed to do this and much more. The first argument `x` is the raster layer. The second argument `y` is the vector layer containing the polygons that you want to extract data over. `include_area = TRUE` tells the function to include the cell area, which does vary by latitude in our case because of [distortions caused by the projection](https://www.researchgate.net/figure/The-area-of-raster-pixels-in-the-WGS84-coordinate-reference-system-expressed-as-a_fig3_368311120). `progress = FALSE` turns off the progress bar.

**Note that this operation took about 5 seconds on my macbook pro and consumes about 200MB of RAM.**

```{r}
# Use prefecture level data to extract land cover cell values for each prefecture
ex_jp_lc <- exact_extract(x = jp_lc, 
                          y = pref, 
                          include_area = TRUE,
                          progress = FALSE)
```

The result is a 47-element list of data frames (one for each prefecture) containing the land cover classification (value), area (in meters squared), and the fraction of a cell inside of the polygon (coverage_fraction) of all raster cells overlapping with polygons. The order of the list elements follows the `pref` as it was input to `exact_extract()`.

### Step 3: Calculate area by land classification

We want to calculate the area covered by each land cover type within each prefecture. Let's start by calculating the area of the first data frame in the list, which happens to be Aichi Ken.

```{r}
aichi <- ex_jp_lc[[1]] %>%
  group_by(value) %>%
  summarise(area_m2 = sum(coverage_fraction * area, na.rm = TRUE)) %>%
  mutate(adm_code = pref$adm_code[1])

print(aichi)
```

The result is the area calculated for each land cover type in Aichi Ken in square meters. See `measurements$unit_conv()` for a set of convenience functions to convert units. Now that we know our method works and is relatively efficient, we can apply it to all of the list elements using a function called `lapply()` or list apply.

**Note that we could have entered `ex_jp_lc` as the argument of `lapply` directly and written the function to accept a dataframe, but we want to be able to reference elements in `pref` too.**

```{r}
# Process results for each polygon
areas_list <- lapply(seq_along(ex_jp_lc), function(i) { #Define an anonymous function to apply to each element i of a vector of 1 to 47 (the length of ex_jp_lc)
  data <- ex_jp_lc[[i]] %>%
    group_by(value) %>% # Group by land cover type
    summarise(area_m2 = sum(coverage_fraction * area, na.rm = TRUE)) %>% # Calculate area
    mutate(adm_code = pref$adm_code[i]) # Add prefecture name (assumes NAME_1 is the column with prefecture names)
})
```

`areas_list` contains data frames for each prefecture with 14 rows for each land type and 3 columns. Since we appended the adm_code and the data frames share the same three variables, we can append them (row-wise):

```{r}
# Combine results for all polygons
pref_areas <- bind_rows(areas_list)
```

`pref_areas` contains the area of each land type in each prefecture in long form. Many functions are optimized for long form data but it can be counter intuitive if it is not familiar. You may want to reshape the data for inspection or sharing.

```{r}
#Reshaping long to wide
pref_areas_wide <- pref_areas %>%
  pivot_wider(id_cols = adm_code,
              names_from = value,
              values_from = area_m2)
```

We have calculated the land cover area by prefecture in 2022. Now we need to do it again for 2018, and we may want to do it for future vintages of the land cover data. We could copy and paste our code but replace the initial raster connection with `jp_lc_2018`. If you find yourself copying and pasting code to redo as task, you should probably write a function.

```{r}
# Lets make it a function
calculate_areas <- function(raster_layer, user_polygons) {
  ex_temp <- exact_extract(raster_layer, 
                         user_polygons, 
                         include_area = TRUE,
                         progress = FALSE)
  
  
  # Process results for each polygon
  areas_list <- lapply(seq_along(ex_temp), function(i) {
    data <- ex_temp[[i]] %>%
      group_by(value) %>% # Group by land cover type
      summarise(area_m2 = sum(coverage_fraction * area, na.rm = TRUE)) %>% # Calculate area
      mutate(adm_code = user_polygons$adm_code[i]) # Add prefecture name (assumes NAME_1 is the column with prefecture names)
  }) %>%
    bind_rows()

}
```

Now we can apply our function to both vintages of data, making the code easier to follow. We also want to apply more informative land classification labels and colors for plotting. Fortunately, the categories and color scheme we created before, apply to the 2018 data as well. Types wetland (13) and greenhouse (14) are ommitted from the 2018 vintage. We should also include more useful prefecture labels.

```{r}
sum_2022 <- calculate_areas(jp_lc_2022,pref) %>%
  mutate(year=2022) 

sum_2018 <- calculate_areas(jp_lc_2018,pref) %>%
  mutate(year=2018) 

pref_lc <- bind_rows(sum_2018,sum_2022) %>%
  inner_join(cat_ref,by = c("value"="ID")) %>%
  inner_join(st_drop_geometry(pref),by = "adm_code")
```

### Step 4: Plotting the difference

Did the area covered by solar panels increase from 2018 to 2022?

```{r}
#Plot the change in solar panels
pref_lc %>%
  filter(value==12) %>%
  ggplot(aes(y=nam,x=area_m2,fill = as_factor(year))) +
  geom_col(position = "dodge")
```

The answer appears to be yes, there is more land covered by solar in 2022 compared to 2018. However, the plot is not very clear. Let's improve the figure by:

- reordering the prefectures by value
- clarifying the axes and legend labels
- adding a descriptive title
- increasing the font size

```{r}
#| fig-height: 8

#Plot the change in solar panels
pref_lc %>%
  filter(value==12) %>%
  ggplot(aes(y=reorder(nam,area_m2),x=conv_unit(area_m2,"m2","hectare"),fill = as_factor(year))) +
  geom_col(position = "dodge") +
  labs(x="Area (hectares)",y=NULL,title = "Solar Coverage in Japan Prefectures") +
  scale_fill_discrete(name = "Year") +
  theme_bw(base_size = 12)

```

### Step 5: FE Regression

We can estimate a simple two-way fixed effects regression to study which land uses are substitutes and complements to solar panels. We want to regress land used for solar panels on other land uses with controls for prefecture and year. First, we need to prepare the data for regression. 

1. Reshape the data from long to wide format so that each land use is a different variable (and clean the names)
2. Collapse the forest variables since they are fairly correlated.

```{r}
#| eval: false

#Prepare data
reg_dat <- pivot_wider(pref_lc,
                       id_cols = c(nam,year),
                       names_from = label,
                       values_from = area_m2) %>%
  janitor::clean_names() %>%
  mutate(df=deciduous_broad_leaved_forest_dbf + deciduous_needle_leaved_forest_dnf,
         ef=evergreen_needle_leaved_forest_enf + evergreen_broad_leaved_forest_ebf)

#Estimate FE OLS regression and store results in m1
m1 <- feols(solar_panel ~ built_up + cropland + paddy_field + bare + df + ef | nam + year,
            data = reg_dat)

#Print table out in console
etable(m1)
```

This simple FE regression suggests that built up land is a complement and crop land is a substitute (the other coefficients are insignificant). Are these results consistent with your intuition? How do we interpret the coefficients?

## Summary

This workshop demonstrated how to process both vector and spatial raster data to answer a question. We cropped a raster layer for easier processing. We extracted raster cell values inside of `sf` polygons of prefectures. We wrote a function to easily repeat the calculations.


